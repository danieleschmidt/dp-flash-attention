name: üêõ Bug Report
description: Report a bug or unexpected behavior
title: "[BUG] "
labels: ["bug", "needs-triage"]
assignees: []

body:
  - type: markdown
    attributes:
      value: |
        Thanks for taking the time to report a bug! Please fill out this form as completely as possible.
        
        **‚ö†Ô∏è Security Notice**: If this bug relates to privacy guarantees or security vulnerabilities, 
        please email security@dp-flash-attention.org instead of filing a public issue.

  - type: checkboxes
    id: security-check
    attributes:
      label: Security Confirmation
      description: Please confirm this is not a security or privacy issue
      options:
        - label: This bug does not involve privacy guarantee violations or security vulnerabilities
          required: true

  - type: textarea
    id: bug-description
    attributes:
      label: Bug Description
      description: A clear and concise description of what the bug is
      placeholder: "What happened?"
    validations:
      required: true

  - type: textarea
    id: expected-behavior
    attributes:
      label: Expected Behavior
      description: What did you expect to happen?
      placeholder: "What should have happened instead?"
    validations:
      required: true

  - type: textarea
    id: reproduction-steps
    attributes:
      label: Steps to Reproduce
      description: Provide detailed steps to reproduce the bug
      placeholder: |
        1. Import dp_flash_attention
        2. Create DPFlashAttention with parameters...
        3. Call method...
        4. See error
    validations:
      required: true

  - type: textarea
    id: minimal-example
    attributes:
      label: Minimal Code Example
      description: Please provide a minimal, complete, and verifiable example
      render: python
      placeholder: |
        import torch
        from dp_flash_attention import DPFlashAttention
        
        # Your minimal example here

  - type: textarea
    id: error-output
    attributes:
      label: Error Output
      description: Full error traceback or unexpected output
      render: text
      placeholder: "Paste the complete error message here"

  - type: dropdown
    id: bug-category
    attributes:
      label: Bug Category
      description: What category does this bug fall into?
      options:
        - Privacy/DP mechanism
        - CUDA kernel
        - Performance
        - Memory usage
        - Installation/Setup
        - Documentation
        - Testing
        - Other
    validations:
      required: true

  - type: input
    id: privacy-params
    attributes:
      label: Privacy Parameters (if applicable)
      description: "epsilon, delta, clip_norm values used"
      placeholder: "epsilon=1.0, delta=1e-5, clip_norm=1.0"

  - type: textarea
    id: environment
    attributes:
      label: Environment Information
      description: Please provide your environment details
      value: |
        - DP-Flash-Attention version:
        - Python version:
        - PyTorch version:
        - CUDA version:
        - GPU model:
        - Operating System:
        - Installation method (pip, source):
      render: markdown
    validations:
      required: true

  - type: dropdown
    id: severity
    attributes:
      label: Bug Severity
      description: How severe is this bug?
      options:
        - Critical (complete failure, privacy violation)
        - High (major functionality broken)
        - Medium (some functionality broken)
        - Low (minor issue, workaround available)
    validations:
      required: true

  - type: textarea
    id: additional-context
    attributes:
      label: Additional Context
      description: Any other context, screenshots, or information that might be helpful
      placeholder: "Add any other context about the problem here"